W1122 21:02:59.778000 1041042 site-packages/torch/distributed/run.py:793] 
W1122 21:02:59.778000 1041042 site-packages/torch/distributed/run.py:793] *****************************************
W1122 21:02:59.778000 1041042 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1122 21:02:59.778000 1041042 site-packages/torch/distributed/run.py:793] *****************************************
2024-11-22 21:05:52,197 - INFO - Found 964 shards for split train
2024-11-22 21:05:52,302 - INFO - Found 1 shards for split val
slurmstepd: error: *** JOB 60550080 ON della-k11g1 CANCELLED AT 2024-11-29T21:06:56 DUE TO TIME LIMIT ***
srun: Job step aborted: Waiting up to 47 seconds for job step to finish.
slurmstepd: error: *** STEP 60550080.1 ON della-k11g1 CANCELLED AT 2024-11-29T21:06:56 DUE TO TIME LIMIT ***
W1129 21:06:56.119000 1041042 site-packages/torch/distributed/elastic/agent/server/api.py:704] Received 15 death signal, shutting down workers
W1129 21:06:56.153000 1041042 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 1041053 closing signal SIGTERM
W1129 21:06:56.176000 1041042 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 1041054 closing signal SIGTERM
W1129 21:06:56.176000 1041042 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 1041055 closing signal SIGTERM
W1129 21:06:56.176000 1041042 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 1041056 closing signal SIGTERM
W1129 21:06:56.177000 1041042 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 1041057 closing signal SIGTERM
W1129 21:06:56.177000 1041042 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 1041058 closing signal SIGTERM
W1129 21:06:56.177000 1041042 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 1041059 closing signal SIGTERM
W1129 21:06:56.178000 1041042 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 1041060 closing signal SIGTERM
W1129 21:06:56.179000 1041042 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 1041053 closing signal SIGTERM
W1129 21:06:56.180000 1041042 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 1041054 closing signal SIGTERM
W1129 21:06:56.180000 1041042 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 1041055 closing signal SIGTERM
W1129 21:06:56.180000 1041042 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 1041056 closing signal SIGTERM
W1129 21:06:56.180000 1041042 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 1041057 closing signal SIGTERM
W1129 21:06:56.180000 1041042 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 1041058 closing signal SIGTERM
W1129 21:06:56.181000 1041042 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 1041059 closing signal SIGTERM
W1129 21:06:56.181000 1041042 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 1041060 closing signal SIGTERM
